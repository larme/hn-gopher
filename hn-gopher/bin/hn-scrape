#!/usr/bin/env python3

import os
import io
import re
import json
import shutil
import logging
import argparse
import textwrap
import tempfile
from time import sleep
from itertools import islice
from datetime import datetime
from logging.handlers import RotatingFileHandler

import requests
from html2text import HTML2Text
from html2text import config as html2text_conf
from unidecode import unidecode

# The "Official" Hacker news API
HN_URL = 'https://hacker-news.firebaseio.com/v0/'

# An unofficial API that's used because it can return all of the comments for
# a story with a single HTTP call.
HN_SEARCH_URL = 'https://hn.algolia.com/api/v1/'


parser = argparse.ArgumentParser()
parser.add_argument('--log-file')
parser.add_argument('--data-dir', default='/var/gopher')
parser.add_argument('--page-size', default=20, type=int)
parser.add_argument('--page-count', default=5, type=int)
parser.add_argument('--line-width', default=67, type=int)
args = parser.parse_args()


# Setup logging
logger = logging.getLogger('hn-scrape')
logger.setLevel(logging.INFO)
if args.log_file:
    handler = RotatingFileHandler(
        filename=args.log_file, maxBytes=5*1024*1024, backupCount=5)
else:
    handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(filename)s:%(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

session = requests.session()


# The API returns comments as formatted HTML so we need to convert to markdown
html_parser = HTML2Text()
html_parser.body_width = 0
html_parser.ignore_links = True

# No need to escape markdown sensitive characters with backslashes because we
# won't actually be rendering the text in a markdown engine.
regex_noop = re.compile(r'a^')
html2text_conf.RE_MD_CHARS_MATCHER = regex_noop
html2text_conf.RE_MD_BACKSLASHMATCHER = regex_noop
html2text_conf.RE_MD_DOT_MATCHER = regex_noop
html2text_conf.RE_MD_PLUS_MATCHER = regex_noop
html2text_conf.RE_MD_DASH_MATCHER = regex_noop


def text_chop(text):
    if len(text) > args.line_width:
        return text[:args.line_width - 3] + '...'
    return text


def text_repeat(char):
    return char * args.line_width


def sanitize(text, is_html=False):
    if not text:
        return ''

    if is_html:
        text = html_parser.handle(text)

    return unidecode(text)


def humanize_timestamp(timestamp):
    timedelta = datetime.utcnow() - datetime.fromtimestamp(timestamp)

    seconds = int(timedelta.total_seconds())
    if seconds < 60:
        return 'moments ago'
    minutes = seconds // 60
    if minutes < 60:
        return '%d minutes ago' % minutes
    hours = minutes // 60
    if hours < 24:
        return '%d hours ago' % hours
    days = hours // 24
    if days < 30:
        return '%d days ago' % days
    months = days // 30.4
    if months < 12:
        return '%d months ago' % months
    years = months // 12
    return '%d years ago' % years


def safe_request(url, retries=5):
    # Have observed that the HN Algolia API sometimes returns 404's for
    # valid URLs. But after a couple of attempts it starts to work again.
    for _ in range(retries):
        try:
            logger.info(url)
            resp = session.get(url)
            resp.raise_for_status()
        except requests.exceptions.HTTPError as e:
            logger.warning(e)
            sleep(1)
        else:
            return resp
    raise RuntimeError('Unable to complete HTTP request, aborting script')


def story_generator(stories):
    for story_id in stories:
        resp = safe_request('{0}item/{1}.json'.format(HN_URL, story_id))
        data = resp.json()
        if data['type'] in ('story', 'job'):
            yield data


def write_gophermap(directory, lines):
    os.makedirs(directory, exist_ok=True)
    gophermap = directory + '/gophermap'
    with io.open(gophermap, 'w+', encoding='ascii', errors='replace') as fp:
        fp.writelines('{0}\n'.format(line) for line in lines)


def dump_raw_story(directory, data):
    os.makedirs(directory, exist_ok=True)
    filename = directory + '/{0}.json'.format(data['id'])
    with io.open(filename, 'w', encoding='utf-8') as fp:
        json.dump(data, fp)


def update_story_file(story, page, path):
    resp = safe_request('{0}items/{1}'.format(HN_SEARCH_URL, story['id']))
    data = resp.json()

    out = [
        '1HN Gopher Live Feed - Page {0}\t/live/p{0}'.format(page),
        'i ',
        'i' + text_repeat('_')
    ]

    title = sanitize(story['title'])
    out.extend('i{0}'.format(l) for l in textwrap.wrap(title, args.line_width))
    info = 'i{0} points by {1}'.format(story['score'], sanitize(story['by']))
    out.append(info)

    if story.get('url'):
        safe_url = text_chop(sanitize(story['url']))
        out.append('h{0}\tURL:{1}'.format(safe_url, story['url']))
    elif story.get('text'):
        # Strip out HTML tags
        text = sanitize(story.get('text'), is_html=True)
        out.append('i ')
        out.extend(['i' + l for l in textwrap.wrap(text, args.line_width)])

    out.extend([
        'i' + text_repeat('_'),
        'i '])

    for child in data['children']:
        append_comment(child, out, level=0)

    item_dir = '{0}/items/{1}'.format(path, story['id'])
    write_gophermap(item_dir, out)
    # dump_raw_story(item_dir, data)


def append_comment(comment, out, level=0):
    indent = 'i' + min(level, 4) * 2 * ' '
    body_indent = indent + '| '

    if comment.get('author'):
        info = indent + '|{0} - {1}'.format(
            sanitize(comment['author']),
            humanize_timestamp(comment['created_at_i']))
        out.append(info)

        # See id 14626305 for an example of a text-less comment
        text = sanitize(comment.get('text'), is_html=True)
        wrapper = textwrap.TextWrapper(
            initial_indent=body_indent,
            subsequent_indent=body_indent,
            # width + 1 because the "i" at the beginning of the line doesn't count
            width=args.line_width + 1,
            drop_whitespace=True,
            replace_whitespace=True,
        )
        for paragraph in text.strip().split('\n\n'):
            text_lines = wrapper.wrap(paragraph)
            out.extend(text_lines)
            out.append(body_indent)
        out.pop()  # Drop the blank line after the last paragraph

    else:
        out.append(indent + '[deleted]')

    out.append('i ')

    for child in comment.get('children', []):
        append_comment(child, out, level=level + 1)


def update_live_feed(path):

    logger.info('Fetching list of top stories')
    resp = safe_request('{0}{1}.json'.format(HN_URL, 'topstories'))
    data = resp.json()

    story_gen = story_generator(data)
    story_ids = []
    for page in range(1, args.page_count + 1):
        logger.info('Building page %s', page)

        # Some gopher browsers (notably the iphone app) don't display an empty
        # comment line unless it contains at least one character, so add a space.
        out = [
            '1Home\t/',
            'i ',
            'iHN GOPHER LIVE FEED - PAGE {0}'.format(page)
        ]

        for story in islice(story_gen, args.page_size):
            story_ids.append(story['id'])
            out.append('i ')

            # Title and URL
            title = text_chop(sanitize(story['title']))
            link = '1{0}\t/live/items/{1}'.format(title[:args.line_width], story['id'])
            out.append(link)

            # Additional info
            if story['type'] == 'job':
                info = 'i{0} points {1} (job listing)'.format(
                    story['score'],
                    humanize_timestamp(story['time']))
            else:
                info = 'i{0} points by {1} {2} ({3} comments)'.format(
                    story['score'],
                    sanitize(story['by']),
                    humanize_timestamp(story['time']),
                    story['descendants']
                )
            out.append(info)

            # Update the story's comment file
            update_story_file(story, page, path)

        if page != args.page_count:
            out.extend([
                'i ',
                'iView next page',
                '1HN Gopher Live Feed - Page {0}\t/live/p{0}'.format(page + 1)])

        page_dir = '{0}/p{1}'.format(path, page)
        write_gophermap(page_dir, out)


# Main
try:
    # Stage everything in a temporary directory
    with tempfile.TemporaryDirectory() as tmp_dir:
        logger.info('Setting up staging directory %s', tmp_dir)
        os.chmod(tmp_dir, 0o755)

        update_live_feed(tmp_dir)

        live_dir = args.data_dir + '/live'

        logger.info('Clearing existing stories')
        shutil.rmtree(live_dir, ignore_errors=True)

        logger.info('Copying staging directory to %s', live_dir)
        shutil.copytree(tmp_dir, live_dir)

except Exception as e:
    logger.exception(e)
    raise e

else:
    logger.info('Finished!')
