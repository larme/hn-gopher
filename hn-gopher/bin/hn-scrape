#!/usr/bin/env python3
import os
import io
import re
import shutil
import logging
import argparse
import textwrap
import subprocess
from tempfile import TemporaryDirectory
from time import sleep
from datetime import datetime
from logging.handlers import RotatingFileHandler
from urllib.parse import urlparse

import requests
from html2text import HTML2Text
from html2text import config as html2text_conf
from unidecode import unidecode

# The "Official" Hacker news API
HN_URL = 'https://hacker-news.firebaseio.com/v0/'

# An unofficial API that's used because it can return all of the comments for
# a story with a single HTTP call.
HN_SEARCH_URL = 'https://hn.algolia.com/api/v1/'

parser = argparse.ArgumentParser()
parser.add_argument('--log-file')
parser.add_argument('--data-dir', default='/var/gopher')
parser.add_argument('--page-size', default=20, type=int)
parser.add_argument('--page-count', default=5, type=int)
parser.add_argument('--line-width', default=67, type=int)
args = parser.parse_args()

logger = logging.getLogger('hn-scrape')

# No need to escape markdown sensitive characters with backslashes because we
# won't actually be rendering the text in a markdown engine.
regex_noop = re.compile(r'a^')
html2text_conf.RE_MD_CHARS_MATCHER = regex_noop
html2text_conf.RE_MD_BACKSLASHMATCHER = regex_noop
html2text_conf.RE_MD_DOT_MATCHER = regex_noop
html2text_conf.RE_MD_PLUS_MATCHER = regex_noop
html2text_conf.RE_MD_DASH_MATCHER = regex_noop


def sanitize(text, is_html=False, truncate=False, width=args.line_width):
    if not text:
        return ''

    if is_html:
        html_parser = HTML2Text()
        html_parser.body_width = 0
        html_parser.ignore_links = True
        text = html_parser.handle(text)

    text = unidecode(text)
    if truncate and len(text) > width:
        text = text[:width - 3] + '...'

    return text


def safe_request(url, retries=5):
    # I have observed that the HN Algolia API sometimes returns 404's for
    # valid URLs. But after a couple of attempts it starts to work again.
    for _ in range(retries):
        try:
            logger.info(url)
            resp = requests.get(url)
            resp.raise_for_status()
        except Exception as e:
            logger.exception(e)
            sleep(1)
        else:
            return resp
    raise RuntimeError('Unable to complete HTTP request, aborting script')


def scrape_website(url):
    try:
        text = subprocess.check_output(
            ['w3m', '-cols', '70', '-no-graph', '-s', '-dump', url],
            universal_newlines=True,
            timeout=5
        )
    except subprocess.TimeoutExpired as e:
        logger.exception(e)
        text = 'Timeout fetching resource'
    except subprocess.CalledProcessError as e:
        logger.exception(e)
        text = e.output
    except Exception as e:
        logger.exception(e)
        text = 'Unable to load page: unexpected exception'

    return text


def humanize_timestamp(timestamp):
    timedelta = datetime.utcnow() - datetime.utcfromtimestamp(timestamp)
    seconds = int(timedelta.total_seconds())
    if seconds < 60:
        return 'moments ago'
    minutes = seconds // 60
    if minutes < 60:
        return '%d minutes ago' % minutes
    hours = minutes // 60
    if hours < 24:
        return '%d hours ago' % hours
    days = hours // 24
    if days < 30:
        return '%d days ago' % days
    months = days // 30.4
    if months < 12:
        return '%d months ago' % months
    years = months // 12
    return '%d years ago' % years


def write_ascii_file(filename, text):
    directory = os.path.dirname(filename)
    os.makedirs(directory, exist_ok=True)
    with io.open(filename, 'w+', encoding='ascii', errors='replace') as fp:
        fp.writelines(text)


def append_comment(comment, lines, level):
    indent = 'i' + min(level, 4) * 2 * ' ' + '| '

    if comment.get('author'):
        lines.append(indent + '{} wrote:'.format(sanitize(comment['author'])))

        # See id 14626305 for an example of a text-less comment
        text = sanitize(comment.get('text', ''), is_html=True)
        wrapper = textwrap.TextWrapper(
            initial_indent=indent,
            subsequent_indent=indent,
            width=args.line_width + 1,
            drop_whitespace=True,
            replace_whitespace=True,
        )
        for paragraph in text.strip().split('\n\n'):
            text_lines = wrapper.wrap(paragraph)
            lines.extend(text_lines)
            lines.append(indent)
        lines.pop()  # Drop the blank line after the last paragraph

    else:
        lines.append(indent + '[deleted]')

    lines.append('i ')

    for child in comment.get('children', []):
        append_comment(child, lines, level + 1)


def generate_comments_page(story, base_dir):
    resp = safe_request(HN_SEARCH_URL + 'items/{}'.format(story['id']))
    data = resp.json()

    title = sanitize(story['title'], truncate=True, width=args.line_width - 12)
    lines = [
        'i[HN Gopher] {}'.format(title),
        'i ',
        'i' + '_' * args.line_width,
        'i ',
    ]
    for child in data['children']:
        append_comment(child, lines, 0)

    lines.append('i' + '_' * args.line_width)
    lines.append('i(page generated {:%Y-%m-%d %H:%M} UTC)'.format(datetime.utcnow()))

    filename = os.path.join(base_dir, 'items', str(story['id']), 'comments', 'gophermap')
    write_ascii_file(filename, '\n'.join(lines))


def generate_story_page(story, base_dir):
    story_dir = os.path.join(base_dir, 'items', str(story['id']))

    title = sanitize(story['title'], truncate=True, width=args.line_width - 12)
    lines = [
        'i[HN Gopher] {}'.format(title),
        'i ',
        'i' + '_' * args.line_width,
        'i ',
    ]

    title = sanitize(story['title'])
    for line in textwrap.wrap(
            title,
            width=args.line_width,
            initial_indent='Title  : ',
            subsequent_indent='         ',
    ):
        lines.append('i' + line)

    author = sanitize(story['by'])
    lines.append('iAuthor : {}'.format(author))
    created = datetime.utcfromtimestamp(story['time'])
    humanized = humanize_timestamp(story['time'])
    lines.append('iDate   : {:%Y-%m-%d %H:%M} UTC ({})'.format(created, humanized))
    lines.append('iScore  : {} points'.format(story['score']))

    text = story.get('text', '-')
    text = sanitize(text, is_html=True)
    for line in textwrap.wrap(
            text,
            width=args.line_width,
            initial_indent='Text   : ',
            subsequent_indent='         ',
    ):
        lines.append('i' + line)

    lines.append('i ')
    url = story.get('url')
    if url:
        hostname = sanitize(urlparse(url).hostname)
        line = 'h[HN Gopher] {} - web link ({})\tURL:{}'
        lines.append(line.format(story['id'], hostname, url))

        logger.info(url)
        dump = sanitize(scrape_website(url))
        dump_filename = os.path.join(story_dir, 'dump.txt')
        write_ascii_file(dump_filename, dump)
        line = '0[HN Gopher] {} - cached page\tdump.txt'
        lines.append(line.format(story['id']))

    line = '1[HN Gopher] {} - comments ({})\tcomments'
    lines.append(line.format(story['id'], story['descendants']))

    lines.append('i ')
    lines.append('i' + '_' * args.line_width)
    lines.append('i(page generated {:%Y-%m-%d %H:%M} UTC)'.format(datetime.utcnow()))

    filename = os.path.join(story_dir, 'gophermap')
    write_ascii_file(filename, '\n'.join(lines))


def generate_listing_page(page_number, story_gen, base_dir):
    logger.info('Generating listing page {}'.format(page_number))

    lines = [
        'i[HN Gopher] Live Feed - Page {}'.format(page_number),
        'i ',
        'i' + '_' * args.line_width
    ]
    for _ in range(args.page_size):
        story = next(story_gen)

        generate_story_page(story, base_dir)
        generate_comments_page(story, base_dir)

        title = sanitize(story['title'], truncate=True)
        link = '1{}\t/live/items/{}'.format(title, story['id'])
        lines.append('i ')
        lines.append(link)

        if story['type'] == 'job':
            lines.append('i{} points (job listing)'.format(story['score']))
        else:
            lines.append('i{} points by {} ({} comments)'.format(
                story['score'],
                sanitize(story['by']),
                story['descendants'],
            ))

    if page_number != args.page_count:
        lines.append('i ')
        lines.append('1Go to the next page\t/live/p{}'.format(page_number + 1))

    lines.append('i ')
    lines.append('i' + '_' * args.line_width)
    lines.append('i(page generated {:%Y-%m-%d %H:%M} UTC)'.format(datetime.utcnow()))

    filename = os.path.join(base_dir, 'p{}'.format(page_number), 'gophermap')
    write_ascii_file(filename, '\n'.join(lines))


def story_generator():
    logger.info('Fetching list of top stories')
    resp = safe_request(HN_URL + 'topstories.json')
    story_ids = resp.json()
    for story_id in story_ids:
        resp = safe_request(HN_URL + 'item/{}.json'.format(story_id))
        data = resp.json()
        if data['type'] in ('story', 'job'):
            yield data


def setup_logging():
    if args.log_file:
        filesize = 5 * 1024 * 1024
        handler = RotatingFileHandler(filename=args.log_file, maxBytes=filesize, backupCount=5)
    else:
        handler = logging.StreamHandler()

    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(filename)s:%(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)


def main():
    setup_logging()
    with TemporaryDirectory() as tmp_dir:
        logger.info('Setting up staging directory {}'.format(tmp_dir))
        os.chmod(tmp_dir, 0o755)

        story_gen = story_generator()
        for page_number in range(1, args.page_count + 1):
            generate_listing_page(page_number, story_gen, tmp_dir)

        live_dir = os.path.join(args.data_dir, 'live')
        logger.info('Clearing existing stories')
        shutil.rmtree(live_dir, ignore_errors=True)
        logger.info('Copying staging directory to {}'.format(live_dir))
        shutil.copytree(tmp_dir, live_dir)


if __name__ == '__main__':
    main()
