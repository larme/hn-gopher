#!/usr/bin/env python3

import os
import io
import json
import shutil
import logging
import argparse
import textwrap
import tempfile
from time import sleep
from itertools import islice
from datetime import datetime
from logging.handlers import RotatingFileHandler

import requests
from lxml import html
from unidecode import unidecode

HN_URL = 'https://hacker-news.firebaseio.com/v0/'
# This API is nice because it can return all comments for a story with a
# single HTTP call
HN_SEARCH_URL = 'https://hn.algolia.com/api/v1/'


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--log-file')
    parser.add_argument('--data-dir', default='/var/gopher')
    parser.add_argument('--page-size', default=10)
    parser.add_argument('--page-count', default=10)
    parser.add_argument('--line-width', default=67)
    return parser.parse_args()


def setup_log(filename):
    logger = logging.getLogger('hn-scrape')
    logger.setLevel(logging.INFO)

    if filename:
        handler = RotatingFileHandler(
            filename=filename, maxBytes=5*1024*1024, backupCount=5)
    else:
        handler = logging.StreamHandler()

    fmt = logging.Formatter('%(asctime)s:%(levelname)s:%(filename)s:%(message)s')
    handler.setFormatter(fmt)
    logger.addHandler(handler)
    return logger


def sanitize(text):
    # 7-bit ASCII was good enough for 1991, so it's good enough for me
    # This package helps convert things like smart quotes to plain ascii
    return unidecode(text)


def humanize_timestamp(timestamp):
    timedelta = datetime.utcnow() - datetime.fromtimestamp(timestamp)

    seconds = int(timedelta.total_seconds())
    if seconds < 60:
        return 'moments ago'
    minutes = seconds // 60
    if minutes < 60:
        return '%d minutes ago' % minutes
    hours = minutes // 60
    if hours < 24:
        return '%d hours ago' % hours
    days = hours // 24
    if days < 30:
        return '%d days ago' % days
    months = days // 30.4
    if months < 12:
        return '%d months ago' % months
    years = months // 12
    return '%d years ago' % years


def safe_request(url, retries=5):
    # Have observed that the HN Algolia API sometimes returns 404's for
    # valid URLs. But after a couple of attempts it starts to work again.
    for _ in range(retries):
        try:
            _logger.info(url)
            resp = session.get(url)
            resp.raise_for_status()
        except requests.exceptions.HTTPError as e:
            _logger.warning(e)
            sleep(1)
        else:
            return resp
    raise RuntimeError('Unable to complete HTTP request, aborting script')


def story_generator(stories):
    for story_id in stories:
        resp = safe_request('{0}item/{1}.json'.format(HN_URL, story_id))
        data = resp.json()
        if data['type'] in ('story', 'job'):
            yield data


def write_gophermap(directory, lines):
    os.makedirs(directory, exist_ok=True)
    gophermap = directory + '/gophermap'
    with io.open(gophermap, 'w+', encoding='ascii', errors='replace') as fp:
        fp.writelines('{0}\n'.format(line) for line in lines)


def dump_raw_story(directory, data):
    os.makedirs(directory, exist_ok=True)
    filename = directory + '/{0}.json'.format(data['id'])
    with io.open(filename, 'w', encoding='utf-8') as fp:
        json.dump(data, fp)


def update_story_file(story, page):
    resp = safe_request('{0}items/{1}'.format(HN_SEARCH_URL, story['id']))
    data = resp.json()

    out = [
        '1HN Gopher Live Feed - page {0} of {1}\t/live/p{0}'.format(page, args.page_count),
        'i ',
        'i' + text_repeater('_')]

    title = sanitize(story['title'])
    out.extend('i{0}'.format(l) for l in text_wrapper.wrap(title))
    info = 'i{0} points by {1}'.format(story['score'], sanitize(story['by']))
    out.append(info)

    if story.get('url'):
        safe_url = text_chop(sanitize(story['url']))
        out.append('h{0}\tURL:{1}'.format(safe_url, story['url']))
    elif story.get('text'):
        # Strip out HTML tags
        doc = html.fromstring(story['text'])
        text = sanitize(doc.text_content())
        out.append('i ')
        out.extend(['i' + l for l in text_wrapper.wrap(text)])

    out.extend([
        'i' + text_repeater('_'),
        'i '])

    for child in data['children']:
        append_comment(child, out, level=0)

    item_dir = '{0}/items/{1}'.format(tmp_dir, story['id'])
    write_gophermap(item_dir, out)
    # dump_raw_story(item_dir, data)


def append_comment(comment, out, level=0):
    indent = 'i' + min(level, 5) * 2 * ' '

    if comment.get('author'):
        info = indent + '{0} - {1}'.format(
            sanitize(comment['author']),
            humanize_timestamp(comment['created_at_i']))
        out.append(info)

        # The text field contains HTML markup like <p> and <a>,
        # as well as HTML escaped characters.
        if comment['text']:
            doc = html.fromstring(comment['text'])
            text = doc.text_content()
        else:
            # See id 14626305 for an example of a text-less comment
            text = ''

        wrapper = textwrap.TextWrapper(
            initial_indent=indent,
            subsequent_indent=indent,
            width=args.line_width + 1)  # +1 for the prepended "i"
        text_lines = wrapper.wrap(text)

        out.extend(text_lines)
    else:
        out.append(indent + '[deleted]')

    out.append('i ')

    for child in comment.get('children', []):
        append_comment(child, out, level=level + 1)


def update_live_feed():

    _logger.info('Fetching list of top stories')
    resp = safe_request('{0}{1}.json'.format(HN_URL, 'topstories'))
    data = resp.json()

    story_gen = story_generator(data)
    story_ids = []
    for page in range(1, args.page_count + 1):
        _logger.info('Building page %s', page)

        # Some gopher browsers (notably the iphone app) don't display an empty
        # comment line unless it contains at least one character, so add a space.
        out = ['1HN Gopher Home Page\t/'.format(page)]

        if page != 1:
            out.extend([
                'i ',
                'i(previous)',
                '1HN Gopher Live Feed - page {0} of {1}\t/live/p{0}'.format(page - 1, args.page_count)])

        for story in islice(story_gen, args.page_size):
            story_ids.append(story['id'])
            out.append('i ')

            # Title and URL
            title = text_chop(sanitize(story['title']))
            if story.get('url'):
                out.append('h{0}\tURL:{1}'.format(title, story['url']))
            else:
                out.append('i{0}'.format(title))

            # Additional info
            if story['type'] == 'job':
                info = 'i{0} points {1}'.format(
                    story['score'],
                    humanize_timestamp(story['time']))
            else:
                info = 'i{0} points by {1} {2}'.format(
                    story['score'],
                    sanitize(story['by']),
                    humanize_timestamp(story['time']))
            out.append(info)

            # Link to comments
            if story['type'] == 'job':
                comments = 'view listing - {0}'.format(title)
                link = '1{0}\t/live/items/{1}'.format(comments[:args.line_width], story['id'])
                out.append(link)
            else:
                comments = 'view comments ({0}) - {1}'.format(story['descendants'], title)
                link = '1{0}\t/live/items/{1}'.format(comments[:args.line_width], story['id'])
                out.append(link)

            # Update the story's comment file
            update_story_file(story, page)

        if page != args.page_count:
            out.extend([
                'i ',
                'i(more)',
                '1HN Gopher Live Feed - page {0} of {1}\t/live/p{0}'.format(page + 1, args.page_count)])

        page_dir = '{0}/p{1}'.format(tmp_dir, page)
        write_gophermap(page_dir, out)


##############################################################################
# Main
##############################################################################
args = parse_args()

text_wrapper = textwrap.TextWrapper(args.line_width)
text_repeater = lambda char: char * args.line_width


def text_chop(text):
    if len(text) > args.line_width:
        return text[:args.line_width - 3] + '...'
    return text


session = requests.session()
_logger = setup_log(args.log_file)

try:
    # Stage everything in a temporary directory
    with tempfile.TemporaryDirectory() as tmp_dir:
        _logger.info('Setting up staging directory %s', tmp_dir)
        os.chmod(tmp_dir, 0o755)

        update_live_feed()

        live_dir = args.data_dir + '/live'

        _logger.info('Clearing existing stories')
        shutil.rmtree(live_dir, ignore_errors=True)

        _logger.info('Copying staging directory to %s', live_dir)
        shutil.copytree(tmp_dir, live_dir)

except Exception as e:
    _logger.exception(e)
    raise e

else:
    _logger.info('Finished!')
